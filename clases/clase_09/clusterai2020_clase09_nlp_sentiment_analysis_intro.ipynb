{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "__Universidad Tecnológica Nacional, Buenos Aires__<br/>\n",
    "__Ingeniería Industrial__<br/>\n",
    "__Cátedra de Ciencia de Datos - Curso I5521 - Turno sabado mañana__<br/>\n",
    "__Docente: Martin Palazzo__\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSzrJf2tVow8"
   },
   "source": [
    "# Practica de Procesamiento de Lenguaje Natural: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zY-VqKKYVow9"
   },
   "source": [
    "**Dataset de titulares sobre Juegos Olimpicos**: Generamos el dataset en el mismo script. Cada documento es un titular respecto de los JJOO 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9XOyPmcAwV5"
   },
   "outputs": [],
   "source": [
    "document_0 = \"Facundo Díaz Acosta se metió en la final y aseguró otra medalla argentina\"\n",
    "document_1 = \"Juegos Olímpicos de la Juventud: una medalla segura para Argentina\"\n",
    "document_2 = \"El tenis argentino se aseguró una medalla en los Juegos Olímpicos de la Juventud gracias a Facundo Díaz Acosta\"\n",
    "document_3 = \"Buenos Aires 2018 Juegos Olímpicos de la Juventud: el BMX aportó el segundo oro para Argentina\"\n",
    "document_4 = \"Agustina Roth e Iñaki Iriartes Mazza le dieron a Argentina su segundo oro en Buenos Aires 2018. Fue en el Ciclismo BMX Freestyle Mixto.\"\n",
    "document_5 = \"Facu va por el oro\"\n",
    "document_6 = \"Argentina sumó otra medalla: Firmapaz se llevó el bronce en tiro\"\n",
    "document_7 = \"Plata para el mexicano Edson Ramírez El mexicano Edson Ramírez obtuvo la medalla de plata al participar en equipo mixto con la rusa Anastasiia Dereviagina\"\n",
    "document_8 = \"Nadadora mexicana Sansores es semifinalista en Buenos Aires 2018\"\n",
    "document_9 = \"Julieta Lema continúa haciendo historia: es semifinalista de los 50 metros\"\n",
    "document_10 = \"Puntaje perfecto para el beach handball masculino\"\n",
    "document_11 = \"Las diez promesas argentinas con chances de medalla en Buenos Aires 2018\"\n",
    "document_12 = \"Primera derrota para las Kamikazes, que están en el hexagonal final\"\n",
    "document_13 = \"Hay que remar para tener el oro\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos todos los textos en un array de numpy llamado \"X_deporte\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxZIlkYHVoxB"
   },
   "outputs": [],
   "source": [
    "x_deporte = [document_0, document_1, document_2, document_3, document_4, document_5, document_6, document_7, document_8, document_9, document_10, document_11, document_12, document_13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kui6qx-1VoxE",
    "outputId": "8c1eb2cf-fb22-455b-fb03-5b74a3e95f62"
   },
   "outputs": [],
   "source": [
    "print(\"El dataset de deportes tiene \" + str(np.shape(x_deporte)[0]) + \" titulares.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACgFj8aPVoxH"
   },
   "source": [
    "**Dataset de titulares sobre clima y tormentas**: generamos en el script de jupyter un dataset con los titulares de desastres naturales y tormentas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1nrIh9kAwV9"
   },
   "outputs": [],
   "source": [
    "document_14 = \"El huracán Michael de categoría 4 toca tierra en Mexico Beach como la peor tormenta en llegar al noroeste de Florida en más de un siglo\"\n",
    "document_15 = \"Michael se degradó a tormenta tropical tras sembrar devastación en Florida\"\n",
    "document_16 = \"El sistema para catalogar huracanes quedó en el ojo de la tormenta\"\n",
    "document_17 = \"Las Carolinas se preparan para daños catastróficos.\"\n",
    "document_18 = \"La increíble imagen de los vientos de la tormenta Florence\"\n",
    "document_19 = \"¿Cómo se desencadenó la mortal tormenta en Mallorca?\"\n",
    "document_20 = \"Un experto meteorólogo, sobre la riada en Mallorca: Predecir algo así es imposible\"\n",
    "document_21 = \"Aemet investiga un posible tornado en La Mancha\"\n",
    "document_22 = \"Un tornado de fuego sorprendió en medio de la lucha contra el incendio en Traslasierra que ya amenaza a Calamuchita\"\n",
    "document_23 = \"SORPRENDENTES IMÁGENES DE TORNADO QUE SE FORMÓ EN SINALOA\"\n",
    "document_24 = \"Un tornado provoca graves daños cerca de la capital de Canadá\"\n",
    "document_25 = \"Espectaculares imágenes del tifón Trami desde el espacio\"\n",
    "document_26 = \"El deporte de perseguir una tormenta y un tornado\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos todos los textos en un array de numpy llamado \"x_clima\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SlSIUyaYVoxK"
   },
   "outputs": [],
   "source": [
    "x_clima = [document_14, document_15, document_16, document_17, document_18, document_19, document_20, document_22, document_23, document_24, document_25, document_26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uSbB4wXrVoxM",
    "outputId": "ee398484-4c65-4100-e41d-ae87ff5b3e30"
   },
   "outputs": [],
   "source": [
    "print(\"El dataset de clima tiene \" + str(np.shape(x_clima)[0]) + \" titulares.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HaDA7gsXVoxO"
   },
   "source": [
    "## Creamos nuestro dataset para trabajar con Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenamos con numpy los arrays de los datos de cada clase en un array llamado \"data_corpus\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CioDxrbcVoxP"
   },
   "outputs": [],
   "source": [
    "data_corpus = np.concatenate((x_clima, x_deporte))\n",
    "data_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos deducir de qué temática es cada grupo de titulares, uno es de JJOO y otro es de clima. Entonces podemos crear las etiquetas de cada clase (y_deporte & y_clima). Una vez creadas podemos concatenarlas en un único vector de etiquetas/labels \"y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZdQaG5IZAwWA"
   },
   "outputs": [],
   "source": [
    "y_deporte = np.full((np.shape(x_deporte)[0],1),1) # etiquetado con \"1\"\n",
    "y_clima = np.full((np.shape(x_clima)[0],1),0) # etiquetado con \"0\"\n",
    "y = np.concatenate((y_clima, y_deporte))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0nhvs9UVoxV"
   },
   "source": [
    "## Bag of Words con CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método Bag of Words como vimos consiste en transformar nuestra fuente original de datos (textos) en una matriz cuyas filas corresponderán a un documento (titular) y sus columnas a los tokens definidos (en este caso cada token es una palabras únicas). El contenido de la matriz obtenida por el CountVectorizer (Bag of Words) representa la frecuencia en que un token aparece en cada documento. Es decir si el token/palabra \"j\" aparece 10 veces en el documento \"i\" entonces en la posición \"Xij\" el valor sera 10. De todos modos la matriz sera mayormente \"sparse\" dado que estara conformada mayoritariamente por ceros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Importamos el Countvectorizer de Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnQfpo-RAwWL"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Creamos un elemento \"cvec\" para realizar el BoW. Ver que se contempla el lowercase = False, esto quiere decir que las palabras en mayúsculas serán consideradas distintas a las mismas escritas en minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smz0Cqa6AwWO"
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(lowercase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Aplicando \".fit_transform\" con \"cvec\" a nuestro dataset original \"data_corpus\" obtendremos una matriz llamada \"xbow\" donde se encontrará la matriz con las transformaciones que ofrece el BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xn3Q7U3tAwWW"
   },
   "outputs": [],
   "source": [
    "xbow = cvec.fit_transform(data_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) ver que el resultado del CountVectorizer es una matriz sparse. Esto es una forma de ofrecer los resultados cuando la matriz resultante tiene muchos ceros (se dice que es sparse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qFpBOuQeAwWX",
    "outputId": "8dc4f7a9-2abe-44a7-802b-309e6a992db8"
   },
   "outputs": [],
   "source": [
    "xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Si quisiéramos obtener nuestra matriz en formato array clásico entonces tenemos que aplicar el comando \".toarray()\" a la matriz sparse. Observaremos que aparece la matriz con muchos ceros. Podemos verificar que los renglones coincide con la cantidad de documentos mientras que las columnas coincide con la cantidad de tokens generados por el CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Km57BSUAwWc",
    "outputId": "41a96b79-2f51-4e38-ed94-587917f1402e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xbow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisiéramos obtener los tokens generados por el CountVectorizer (BoW) debemos ejecutar el comando \".get_features_names()\" al \"cvec\" ya fiteado con los datos originales. Guardamos los tokens obtenidos en el elemento \"tokens_bow\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwC4HJ8OAwWe"
   },
   "outputs": [],
   "source": [
    "tokens_bow = cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QbKV6XBAwWh",
    "outputId": "df37be46-be25-48f1-fa24-e52dd5304eba"
   },
   "outputs": [],
   "source": [
    "print(tokens_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kAxhZHYVoxm"
   },
   "source": [
    "## TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una alternativa al BoW es el TF-IDF. Como vimos en la teoria, este procesador de texto le asigna un score mas alto a los tokens (palabras) que son frecuentes pero únicamente en pocos documentos, es decir aquellos tokens que son informativos. Por otro lado, si una palabra es frecuente en un documento aunque esta presente en varios documentos del dataset, entonces el score es bajo o nulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Importamos el TfidfVectorizer de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APjai2upAwWk"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Declaramos un elemento llamado tdidfvec que consistirá en un procesador de texto TD-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MI1fVMRqAwWn"
   },
   "outputs": [],
   "source": [
    "tfidfvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Transformamos nuestro dataset original con un TFIDF y obtenemos una matriz llamada \"x_tfidf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCepBh_AAwWp",
    "outputId": "2e5d3e0c-fbe2-4573-8907-77c7ce2498dd"
   },
   "outputs": [],
   "source": [
    "x_tfidf = tfidfvec.fit_transform(data_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Observamos los tokens obtenidos con el tfidf vectorizer mediante el comando \".get_features_names()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZStZgcE1AwWs",
    "outputId": "af3f59db-b91d-4979-a7ad-24243e9f5acf"
   },
   "outputs": [],
   "source": [
    "# imprimimos los tokens en pantalla\n",
    "print(tfidfvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Visualizamos la matriz obtenida luego de transformar nuestros datos originales con \"tf_idf\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVUBxc0RAwWw",
    "outputId": "1e9917f7-599d-4fd0-bf3a-9386f2b2ef99"
   },
   "outputs": [],
   "source": [
    "# observamos la dimension de los datos de texto transformados a matriz/vector\n",
    "print(x_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Como los datos obtenidos se encuentran en una matriz sparse, utilizamos el comando \".toarray()\" para transformarlos en un array clásico. A diferencia del BagOfWords en vez de haber simplemente la frecuencia (cantidad de veces que aparecio el token), en este caso el TF-IDF calcula un indicador mas complejo y mas informativo sobre la importancia de cada token en un documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOwqjx3JAwWz",
    "outputId": "316eda26-f50b-425d-9aea-7fc80d30e24b"
   },
   "outputs": [],
   "source": [
    "# transformamos la matriz sparse \"xt\"\n",
    "x_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V10IYFpTVox1"
   },
   "source": [
    "## Visualizacion con PCA (principal component analysis) de los datos obtenidos con TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos PCA para poder visualizar nuestros datos en 2 dimensiones, es decir, utilizando las primeras 2 componentes del PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zqaP0nCAwW5",
    "outputId": "69ec4ef2-a426-4a29-adf4-35b6642e2a2d"
   },
   "outputs": [],
   "source": [
    "# transformamos nuestros datos originales a 2 dimensiones nuevas creadas por el PCA. \n",
    "# estas nuevas dimensiones tratan de captar la mayor variabilidad de los datos (son dimensiones nuevas).\n",
    "xpca = PCA(n_components=2).fit_transform(x_tfidf.toarray())\n",
    "np.shape(xpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSEv3aVYAwW8"
   },
   "source": [
    "Ahora que los datos los tenemos en 2 dimensiones podemos graficarlos para visualizar con un scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJIVqRuqAwXC",
    "outputId": "e66938d4-1302-4e38-99d8-6a26e16ff882"
   },
   "outputs": [],
   "source": [
    "# realizamos un scatter plot. \n",
    "plt.scatter(xpca[y.ravel() == 1,0], xpca[y.ravel() == 1,1])\n",
    "plt.scatter(xpca[y.ravel() == 0,0], xpca[y.ravel() == 0,1])\n",
    "plt.title(\"Visualizacion de datos de JJOO vs Clima\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximos pasos: Construir y entrenar un modelo clasificador de texto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestros datos de forma vectorial (x_tfidf) y etiquetados (y) podemos entrenar y evaluar un modelo clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarea:** Entrenar un modelo clasificador utilizando los datos transformados con TF-IDF, 50% para train-validation y el resto para test. Reportar la performance del modelo con Accuracy, Matriz de confusion y AUC ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iDec0DEDVoyG"
   },
   "outputs": [],
   "source": [
    "#### Codigo aqui ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Codigo aqui ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Codigo aqui ####"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "acamica_clase_30.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
